{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81784bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pyvista as pv\n",
    "import pye57\n",
    "import laspy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd14b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "def prepare_dataset(source, target,voxel_size):\n",
    "\n",
    "    trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    source.transform(trans_init)\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh\n",
    "\n",
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 1\n",
    "    result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "        distance_threshold,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "        3, [\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "                0.9),\n",
    "            o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "                distance_threshold)\n",
    "        ], o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999))\n",
    "    return result\n",
    "\n",
    "def estimate_std(pointCloud, distance_threshold):\n",
    "    pcd = np_point_cloud2_pcd(pointCloud)\n",
    "    _, inliers = pcd.segment_plane(\n",
    "        distance_threshold=distance_threshold, ransac_n=5, num_iterations=1000\n",
    "    )\n",
    "    pcd_inliers = pcd.select_by_index(inliers)\n",
    "    dists = pcd.compute_point_cloud_distance(pcd_inliers)\n",
    "    dists = np.asarray(dists)\n",
    "    return 1 - np.std(dists)\n",
    "\n",
    "def request_points(data, reference_data, buff):\n",
    "    # data = data.reshape(-1,6)\n",
    "    noise_point = 1\n",
    "    # print(data.shape)\n",
    "    points = data[\n",
    "        np.where(\n",
    "            (data[:, 0] < reference_data[0] + buff)\n",
    "            & (data[:, 0] > reference_data[0] - buff)\n",
    "            & (data[:, 1] < reference_data[1] + buff)\n",
    "            & (data[:, 1] > reference_data[1] - buff)\n",
    "            & (data[:, 2] < reference_data[2] + buff)\n",
    "            & (data[:, 2] > reference_data[2] - buff)\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "    points = points.reshape(-1, 6)\n",
    "    if points.shape[0] > 10:\n",
    "        noise_point = estimate_std(points, buff * 0.01)\n",
    "    return points, noise_point\n",
    "\n",
    "def compute_rmse(lst_PC,lst_acc,voxel_size):\n",
    "    PC_list = lst_PC.copy()\n",
    "    target =  np_point_cloud2_pcd(PC_list.pop(lst_acc.index(min(lst_acc))))\n",
    "    rmse = []\n",
    "    lst_pcs = []\n",
    "    PC_list = lst_PC.copy()\n",
    "    for pc in PC_list:\n",
    "        source =  np_point_cloud2_pcd(pc)\n",
    "\n",
    "        source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(source, target,voxel_size)\n",
    "        result_ransac = execute_global_registration(source_down, target_down,\n",
    "                                                    source_fpfh, target_fpfh,\n",
    "                                                    voxel_size)\n",
    "        rmse.append(result_ransac.inlier_rmse)\n",
    "        lst_pcs.append( pcd2np_point_cloud(source.transform(result_ransac.transformation)))\n",
    "        # print(result_ransac.inlier_rmse)\n",
    "    return lst_pcs, np.array(rmse)\n",
    "\n",
    "\n",
    "def check_points_distribution(selected_point, reference_point, threshold, voxel_size):\n",
    "    selected_point = selected_point.reshape(-1, 6)\n",
    "    if selected_point.shape[0] > 1:\n",
    "        return np.linalg.norm(\n",
    "            np.mean(selected_point, axis=0)[:3] - reference_point[:3]\n",
    "        ) > (threshold * voxel_size)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def adjusted_voxel2(ds_sub_reference,ds_reference_pnt,super_voxel_points, g_weights,buff,threshold,point_cont):\n",
    "    # point_cont = np.zeros(len(super_voxel_points))\n",
    "    fused_sub_point = np.zeros((1,6))\n",
    "    sub_voxel_points,_ =  request_points(ds_sub_reference,ds_reference_pnt,buff)\n",
    "    for i in range(sub_voxel_points.shape[0]):\n",
    "        sub_vox_pnt = []\n",
    "        l_weights = []\n",
    "        for data in range(len(super_voxel_points)):\n",
    "            points, noise =  request_points(super_voxel_points[data],sub_voxel_points[i],buff*0.5)\n",
    "            weight_l = points.shape[0]*noise\n",
    "            l_weights.append(weight_l)\n",
    "            sub_vox_pnt.append(points)\n",
    "        weights = np.array(np.array(g_weights)*np.array(l_weights))\n",
    "        weights = weights/np.sum(weights)\n",
    "        point = sub_vox_pnt[np.argmax(weights)]\n",
    "        # print(np.argmax(weights))\n",
    "        point_cont[np.argmax(weights)] = point_cont[np.argmax(weights)]+1\n",
    "\n",
    "        if  check_points_distribution(point,sub_voxel_points[i],threshold,buff):\n",
    "            point = np.vstack(sub_vox_pnt)\n",
    "            point_cont = point_cont+1\n",
    "        fused_sub_point = np.concatenate((fused_sub_point,point.reshape(-1,6)),axis=0)\n",
    "    return fused_sub_point[1:,:],point_cont\n",
    "\n",
    "\n",
    "def weighted_fusion_filter(data_list,g_weight,reference_pc,sub_reference_pc,voxel_size,k_1,k_2, threshold):\n",
    "    point_cont = np.zeros(len(data_list))\n",
    "    fused_points = np.zeros((1,6))\n",
    "    buff = (voxel_size+voxel_size*0.1)/2\n",
    "    for i in tqdm.tqdm(range(reference_pc.shape[0])):\n",
    "    # for i in range(6):\n",
    "        points = []\n",
    "        l_weights = []\n",
    "        for data in range(len(data_list)):\n",
    "            pnts,noise_point =  request_points(data_list[data],reference_pc[i],buff)\n",
    "            points.append(pnts)\n",
    "            weight = (pnts.shape[0]/(voxel_size*10)**3)*noise_point\n",
    "            l_weights.append(weight)            \n",
    "        l_weights = l_weights/np.sum(l_weights)\n",
    "        weights = (k_1* np.array(g_weight)) + (k_2*np.array(l_weights))\n",
    "        weights = weights/np.sum(weights)  \n",
    "        point = points[np.argmax(weights)]\n",
    "        \n",
    "\n",
    "        if  check_points_distribution(point,reference_pc[i],threshold,voxel_size):\n",
    "            point, point_cont_ = adjusted_voxel2(sub_reference_pc,reference_pc[i],points,g_weight,buff,0.5*threshold,point_cont)\n",
    "        else:\n",
    "            point_cont[np.argmax(weights)] = point_cont[np.argmax(weights)] + 1\n",
    "    \n",
    "        # point =  opend3d_outlier(point,option=\"statistical\")\n",
    "        fused_points = np.concatenate((fused_points,point.reshape(-1,6)),axis=0)\n",
    "    return fused_points[1:,:],point_cont_\n",
    "\n",
    "def load_PC_acc():\n",
    "    pc_path = str(input(\"Enter point cloud directory: \"))\n",
    "    accuracy = float(input(\"Enter the accuracy of the sensor system in meters: \"))\n",
    "    print(\"Loading point cloud\")\n",
    "    np_pointCloud =  load_pointCloud(pc_path)\n",
    "    print(\"Done.\")\n",
    "    return np_pointCloud, accuracy,pc_path\n",
    "\n",
    "def compute_fusion_accuracy(point_cont,lst_acc):\n",
    "    lst_contrib = point_cont/np.sum(point_cont)\n",
    "    acc_contrib = lst_contrib * (np.array(lst_acc)**2)\n",
    "    # acc_contrib = acc_contrib**2\n",
    "    fusion_acc = np.sqrt(np.sum(acc_contrib))\n",
    "    return lst_contrib,fusion_acc\n",
    "\n",
    "def np_point_cloud2_pcd(point_cloud):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud[:, :3])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(point_cloud[:, 3:6])\n",
    "    pcd.estimate_normals(fast_normal_computation=False)\n",
    "    return pcd\n",
    "\n",
    "def compute_surface_area(pcd, voxel_size):\n",
    "    radii = [voxel_size, voxel_size * 2, voxel_size * 4, voxel_size * 5]\n",
    "    rec_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(\n",
    "        pcd, o3d.utility.DoubleVector(radii)\n",
    "    )\n",
    "    return rec_mesh.get_surface_area()\n",
    "\n",
    "def compute_coverage(reference_pcd, pcd, voxel_size):\n",
    "    ds_ref_pcd = reference_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "    ds_pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "    coverage = compute_surface_area(ds_pcd, voxel_size) / compute_surface_area(\n",
    "        ds_ref_pcd, voxel_size\n",
    "    )\n",
    "    return coverage\n",
    "\n",
    "def weight_compute(reference_pcd, pcd, accuracy, voxel_size):\n",
    "    completeness = compute_coverage(reference_pcd, pcd, voxel_size)\n",
    "    weight = accuracy / completeness\n",
    "    return weight\n",
    "\n",
    "def pcd2np_point_cloud(pcd):\n",
    "    pc_down_point = np.asarray(pcd.points)\n",
    "    pc_down_color = np.asarray(pcd.colors)\n",
    "    np_point_Cloud = np.concatenate((pc_down_point, pc_down_color), axis=1)\n",
    "    return np_point_Cloud\n",
    "\n",
    "def compute_weights(reference_pc, pc_list, accuracy, voxel_size):\n",
    "    weights = []\n",
    "    reference_pcd = np_point_cloud2_pcd(reference_pc)\n",
    "    for data in range(len(pc_list)):\n",
    "        data_pcd = np_point_cloud2_pcd(pc_list[data])\n",
    "        weights.append(\n",
    "            weight_compute(reference_pcd, data_pcd, accuracy[data], voxel_size * 2)\n",
    "        )\n",
    "        data_pcd = []\n",
    "    if len(weights) > 1:\n",
    "        weights = np.sum(weights) - weights\n",
    "\n",
    "    weights_ = weights / np.sum(weights)\n",
    "    lst_PCs, rmse= compute_rmse(pc_list,accuracy,voxel_size) \n",
    "    g_weight = weights/(1-rmse)\n",
    "    return g_weight\n",
    "\n",
    "\n",
    "###############\n",
    "# Point cloid IO\n",
    "\n",
    "def load_pointCloud(pc_path):\n",
    "    pc_ext = pc_path.split(\".\")[-1]\n",
    "    if pc_ext == \"e57\":\n",
    "        np_pointCloud = load_pye57_file(pc_path)\n",
    "    elif pc_ext == \"las\" or pc_ext == \"laz\":\n",
    "        np_pointCloud = load_las_file(pc_path)\n",
    "    elif pc_ext == \"ply\":\n",
    "        np_pointCloud = load_ply_file(pc_path)\n",
    "    else:\n",
    "        print(\n",
    "            \"ERROR: Check the file extension of the file. \\n Only accepts *.ply, *.las/*.laz and *.e57En\"\n",
    "        )\n",
    "    return np_pointCloud\n",
    "\n",
    "def load_las_file(path, intensity=False):\n",
    "    with laspy.open(path) as las_path:\n",
    "        las = las_path.read()\n",
    "\n",
    "    \"\"\" \n",
    "    convert laspy.lasdata.LasData into a np array\n",
    "    \"\"\"\n",
    "\n",
    "    points = np.stack((np.asarray(las.x), np.asarray(las.y), np.asarray(las.z)), axis=1)\n",
    "    try:\n",
    "        colors = np.stack(\n",
    "            (np.asarray(las.red), np.asarray(las.green), np.asarray(las.blue)), axis=1\n",
    "        )\n",
    "        if intensity:\n",
    "            intens = np.asarray(las.intensity)\n",
    "            pc_np = np.concatenate((points, colors, intens), axis=1)\n",
    "        pc_np = np.concatenate((points, colors), axis=1)\n",
    "        return pc_np\n",
    "    except KeyError:\n",
    "        return points\n",
    "    \n",
    "\n",
    "def load_ply_file(path, color=True, estimate_normals=True):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    points = pcd.points\n",
    "    point_cloud = np.array(points)\n",
    "    if color:\n",
    "        colors = pcd.colors\n",
    "        point_cloud = np.concatenate((points, colors), axis=1)\n",
    "\n",
    "    if estimate_normals:\n",
    "        pcd.estimate_normals(fast_normal_computation=False)\n",
    "        normals = pcd.normals\n",
    "        point_cloud = np.concatenate((points, colors, normals), axis=1)\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "def load_pye57_file(path, intensity=False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        path (String): directory to file\n",
    "        intensity (Boolean): intensity measurement from pointcloud\n",
    "\n",
    "    output:\n",
    "        pointcloud (numpy array): point cloud in a numpy array\n",
    "    \"\"\"\n",
    "    e57 = pye57.E57(path)\n",
    "    data = e57.read_scan(\n",
    "        0, colors=True, intensity=intensity, ignore_missing_fields=True\n",
    "    )\n",
    "\n",
    "    points = np.array([data[\"cartesianX\"], data[\"cartesianY\"], data[\"cartesianZ\"]]).T\n",
    "    colors = np.array([data[\"colorRed\"], data[\"colorGreen\"], data[\"colorBlue\"]]).T / 255\n",
    "\n",
    "    # return a numpy array\n",
    "    if intensity:\n",
    "        intensity = np.array([data[\"intensity\"]]).T\n",
    "        point_cloud = np.concatenate((points, colors, intensity), axis=1)\n",
    "\n",
    "    point_cloud = np.concatenate((points, colors), axis=1)\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "def write_las_file(pc_np, las_path):\n",
    "    header = laspy.LasHeader(point_format=2)\n",
    "    xmin = np.floor(np.min(pc_np[:, 0]))\n",
    "    ymin = np.floor(np.min(pc_np[:, 1]))\n",
    "    zmin = np.floor(np.min(pc_np[:, 2]))\n",
    "    header.offset = [xmin, ymin, zmin]\n",
    "    header.scale = [0.001, 0.001, 0.001]\n",
    "\n",
    "    las = laspy.LasData(header)\n",
    "    las.x = pc_np[:, 0]\n",
    "    las.y = pc_np[:, 1]\n",
    "    las.z = pc_np[:, 2]\n",
    "    las.red = pc_np[:, 3] * 255\n",
    "    las.green = pc_np[:, 4] * 255\n",
    "    las.blue = pc_np[:, 5] * 255\n",
    "\n",
    "    if pc_np.shape[1] == 7:\n",
    "        las.intensity = pc_np[:, 6]\n",
    "\n",
    "    las.write(las_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a307a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_pyvista(list_of_pointcloud:list,save_img:bool =False,**kwargs):\n",
    "    filename = kwargs.get(\"filename\",\"filename.png\")\n",
    "    # point_size = kwargs.get(\"point_size\",\"filename.png\")\n",
    "\n",
    "    point_size =1\n",
    "\n",
    "    pl = pv.Plotter()\n",
    "    pl.background_color = 'w' \n",
    "\n",
    "    sargs = dict(\n",
    "            title_font_size=20,\n",
    "            label_font_size=16,\n",
    "            shadow=True,\n",
    "            n_labels=3,\n",
    "            italic=True,\n",
    "            fmt=\"%.1f\",\n",
    "            font_family=\"arial\",\n",
    "            height=0.3, vertical=True, position_x=0.75, position_y=0.2\n",
    "        )\n",
    "\n",
    "    for pc in list_of_pointcloud:\n",
    "        points = pc[:,:3]\n",
    "        if pc.shape[1]>4:\n",
    "            # rgba = pc[:,3:]/np.max(pc[:,3:],axis=0)\n",
    "            rgba = pc[:,3:]\n",
    "            pl.add_points(points,scalars=rgba,rgba=True,point_size=point_size,render_points_as_spheres=True,smooth_shading=True,)\n",
    "\n",
    "        elif pc.shape[1]==4:\n",
    "            scalars = pc[:, -1]\n",
    "            pl.add_points(points, cmap=\"Spectral\", scalars=scalars, point_size=5, scalar_bar_args=sargs)\n",
    "            # pl.add_scalar_bar(\"Coverage Score\",fmt='%10.5f',label_font_size=30,)\n",
    "        else:\n",
    "            rgba = np.ones(pc[:,:3].shape) \n",
    "            point_size = 5\n",
    "            pl.add_points(points,scalars=rgba,rgba=True,point_size=point_size,render_points_as_spheres=True,smooth_shading=True,)\n",
    "\n",
    "\n",
    "    if save_img:\n",
    "        # pl.camera.zoom(1.5)\n",
    "        pl.screenshot(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a59d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mian Paramss\n",
    "voxel_size = 0.05 \n",
    "k_1 = 1\n",
    "k_2 = 1\n",
    "threshold = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca410ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in search space (voxels):  25\n"
     ]
    }
   ],
   "source": [
    "# Load Point Clouds and prepocess data \n",
    "PC1 = \"bun000_Cloud.las\"\n",
    "PC2 = \"bun045_Cloud.las\"\n",
    "output_path = \"\"\n",
    "\n",
    "lst_PC = [load_pointCloud(PC1),load_pointCloud(PC2)]\n",
    "lst_acc = [0.01,0.01]\n",
    "reference_pc = np.vstack(lst_PC)\n",
    "\n",
    "ds_reference = np_point_cloud2_pcd(reference_pc).voxel_down_sample(voxel_size = voxel_size)\n",
    "ds_sub_reference = np_point_cloud2_pcd(reference_pc).voxel_down_sample(voxel_size = voxel_size/2)\n",
    "ds_reference = pcd2np_point_cloud(ds_reference)\n",
    "ds_sub_reference =  pcd2np_point_cloud(ds_sub_reference)\n",
    "print(\"Number of points in search space (voxels): \", ds_reference.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing point cloud weights .... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main Code\n",
    "\n",
    "weights = compute_weights(reference_pc,lst_PC,lst_acc,voxel_size=voxel_size)\n",
    "print(\"Computing point cloud weights .... Done\")\n",
    "\n",
    "fused_PC_1,point_contrib = weighted_fusion_filter(lst_PC,weights,ds_reference,ds_sub_reference,voxel_size,k_1,k_2, threshold)\n",
    "fused_PC_1 = np.unique(fused_PC_1,axis=0)\n",
    "write_las_file(fused_PC_1,os.path.join(output_path,\"weighted_fused_filtered_{}_cm_vox.las\".format(int(voxel_size*100))))\n",
    "fused_PC_1 = []\n",
    "point_contrib = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d558f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
